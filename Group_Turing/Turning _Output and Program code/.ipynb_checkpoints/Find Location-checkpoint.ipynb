{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-14 14:07:46 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: scrapybot)\n",
      "2019-10-14 14:07:46 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.17763-SP0\n",
      "2019-10-14 14:07:46 [scrapy.crawler] INFO: Overridden settings: {}\n",
      "2019-10-14 14:07:46 [scrapy.extensions.telnet] INFO: Telnet Password: 983769c27d7eb02e\n",
      "2019-10-14 14:07:46 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2019-10-14 14:07:46 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-10-14 14:07:46 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-10-14 14:07:46 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-10-14 14:07:46 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-10-14 14:07:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2019-10-14 14:07:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2019-10-14 14:07:47 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://www.klook.com/zh-HK/activity/3-soho-peddler-gallery-tour-hong-kong/?krt=r22&krid=1b4caa02-0e71-42e9-79c8-4184b4c97f2c> (referer: None)\n",
      "2019-10-14 14:07:47 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.klook.com/zh-HK/activity/3-soho-peddler-gallery-tour-hong-kong/?krt=r22&krid=1b4caa02-0e71-42e9-79c8-4184b4c97f2c>: HTTP status code is not handled or not allowed\n",
      "2019-10-14 14:07:52 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://www.klook.com/zh-HK/activity/9-pink-dolphin-sighting-tour-hong-kong/?krt=r22&krid=1b4caa02-0e71-42e9-79c8-4184b4c97f2c> (referer: None)\n",
      "2019-10-14 14:07:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.klook.com/zh-HK/activity/9-pink-dolphin-sighting-tour-hong-kong/?krt=r22&krid=1b4caa02-0e71-42e9-79c8-4184b4c97f2c>: HTTP status code is not handled or not allowed\n",
      "2019-10-14 14:08:07 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://www.klook.com/zh-HK/activity/10-canyoning-adventure-hong-kong/?krt=r22&krid=59ac79a5-b2fd-4758-6c89-e62b19e04fe6> (referer: None)\n",
      "2019-10-14 14:08:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.klook.com/zh-HK/activity/10-canyoning-adventure-hong-kong/?krt=r22&krid=59ac79a5-b2fd-4758-6c89-e62b19e04fe6>: HTTP status code is not handled or not allowed\n",
      "2019-10-14 14:08:07 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force \n",
      "2019-10-14 14:08:07 [scrapy.core.engine] INFO: Closing spider (shutdown)\n",
      "2019-10-14 14:08:12 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy import Request\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "class ProductList(scrapy.Item):\n",
    "    X1_product_name = scrapy.Field()\n",
    "    X2_product_rating= scrapy.Field()\n",
    "\n",
    "#all_items\n",
    "df=pd.read_csv(\"output/links.csv\")['X5_website']\n",
    "class go(scrapy.Spider):\n",
    "    name='test'\n",
    "    allowed_domains=['www.klook.com/zh-HK']\n",
    "    start_urls=['{}'.format(i)for i in df]\n",
    "\n",
    "    def start_requests(self):\n",
    "        headers= {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}\n",
    "        for url in self.start_urls:\n",
    "            yield Request(url, headers=headers)  \n",
    "    \n",
    "    def parse(self,response):\n",
    "        item = ProductList()\n",
    "        target = response.css(\"div.markdown-content ul\")\n",
    "        for tag in target:\n",
    "            try:\n",
    "                if \"地點\" in (tag.css(\"::text\").extract()[0]):\n",
    "                    #print(tag.css(\"::text\").extract()[0:])\n",
    "                    if (len(tag.css(\"::text\").extract()[0:]))>=2:\n",
    "                        if (tag.css(\"::text\").extract()[0]).find(\":\")!=-1:\n",
    "                            k=(tag.css(\"::text\").extract()[0])\n",
    "                        else:\n",
    "                            k=(tag.css(\"::text\").extract()[0]).find(\"：\")\n",
    "                        #print(tag.css(\"::text\").extract()[0])\n",
    "                        print(tag.css(\"::text\").extract()[0][k+1:]) \n",
    "                        \n",
    "                elif \"地址\" in (tag.css(\"::text\").extract()[0]):\n",
    "                    #print(tag.css(\"::text\").extract()[0:])\n",
    "                    if (len(tag.css(\"::text\").extract()[0:]))>=2:\n",
    "                        if (tag.css(\"::text\").extract()[0]).find(\":\")!=-1:\n",
    "                            k=(tag.css(\"::text\").extract()[0])\n",
    "                        else:\n",
    "                            k=(tag.css(\"::text\").extract()[0]).find(\"：\")\n",
    "                        #print(tag.css(\"::text\").extract()[0])\n",
    "                        print(tag.css(\"::text\").extract()[0][k+1:])  \n",
    "            except:\n",
    "                print(\"NA\")\n",
    "                \n",
    "            \n",
    "                \n",
    "process = CrawlerProcess()\n",
    "process.crawl(go)\n",
    "process.start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
